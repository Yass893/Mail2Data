{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exp.Date:</th>\n",
       "      <th>Voyage N°:</th>\n",
       "      <th>Poids:</th>\n",
       "      <th>Montant HT:</th>\n",
       "      <th>filename</th>\n",
       "      <th>Depart</th>\n",
       "      <th>Arrivee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-12-15</td>\n",
       "      <td>PARTRP000100992</td>\n",
       "      <td>6000,00</td>\n",
       "      <td>350,00</td>\n",
       "      <td>SupplierPreInvoiceSPI00450803.pdf</td>\n",
       "      <td>FR-77185 LOGNES\\nA</td>\n",
       "      <td>FR-60300 SENLIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-12-15</td>\n",
       "      <td>PARTRP000101238</td>\n",
       "      <td>769,91</td>\n",
       "      <td>260,00</td>\n",
       "      <td>SupplierPreInvoiceSPI00450803.pdf</td>\n",
       "      <td>FR-95500 BONNEUIL EN\\nA</td>\n",
       "      <td>FR-95130 LE PLESSIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-12-18</td>\n",
       "      <td>PARTRP000101276</td>\n",
       "      <td>184,62</td>\n",
       "      <td>267,50</td>\n",
       "      <td>SupplierPreInvoiceSPI00450803.pdf</td>\n",
       "      <td>FR-95500 BONNEUIL EN\\nA</td>\n",
       "      <td>FR-77400 ST THIBAULT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-12-18</td>\n",
       "      <td>PARTRP000101277</td>\n",
       "      <td>202,33</td>\n",
       "      <td>267,50</td>\n",
       "      <td>SupplierPreInvoiceSPI00450803.pdf</td>\n",
       "      <td>FR-95500 BONNEUIL EN\\nA</td>\n",
       "      <td>FR-91310 MONTLHERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-12-19</td>\n",
       "      <td>PARTRP000101305</td>\n",
       "      <td>3160,00</td>\n",
       "      <td>350,00</td>\n",
       "      <td>SupplierPreInvoiceSPI00450803.pdf</td>\n",
       "      <td>FR-94200 IVRY SUR SEINE\\nA</td>\n",
       "      <td>FR-60300 SENLIS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Exp.Date:       Voyage N°:   Poids: Montant HT:  \\\n",
       "0  2023-12-15  PARTRP000100992  6000,00      350,00   \n",
       "1  2023-12-15  PARTRP000101238   769,91      260,00   \n",
       "2  2023-12-18  PARTRP000101276   184,62      267,50   \n",
       "3  2023-12-18  PARTRP000101277   202,33      267,50   \n",
       "4  2023-12-19  PARTRP000101305  3160,00      350,00   \n",
       "\n",
       "                            filename                      Depart  \\\n",
       "0  SupplierPreInvoiceSPI00450803.pdf          FR-77185 LOGNES\\nA   \n",
       "1  SupplierPreInvoiceSPI00450803.pdf     FR-95500 BONNEUIL EN\\nA   \n",
       "2  SupplierPreInvoiceSPI00450803.pdf     FR-95500 BONNEUIL EN\\nA   \n",
       "3  SupplierPreInvoiceSPI00450803.pdf     FR-95500 BONNEUIL EN\\nA   \n",
       "4  SupplierPreInvoiceSPI00450803.pdf  FR-94200 IVRY SUR SEINE\\nA   \n",
       "\n",
       "                Arrivee  \n",
       "0       FR-60300 SENLIS  \n",
       "1   FR-95130 LE PLESSIS  \n",
       "2  FR-77400 ST THIBAULT  \n",
       "3    FR-91310 MONTLHERY  \n",
       "4       FR-60300 SENLIS  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"C:\\Projet 3\\extracted_data_pdfplumber.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp.Date:       object\n",
      "Voyage N°:      object\n",
      "Poids:          object\n",
      "Montant HT:     object\n",
      "filename        object\n",
      "Depart          object\n",
      "Arrivee         object\n",
      "Depart_Lat     float64\n",
      "Depart_Lng     float64\n",
      "Arrivee_Lat    float64\n",
      "Arrivee_Lng    float64\n",
      "Distance       float64\n",
      "dtype: object\n",
      "\n",
      "Affichage (les  premières lignes) \n",
      "    Exp.Date:       Voyage N°:   Poids: Montant HT:  \\\n",
      "0  2023-12-15  PARTRP000100992  6000,00      350,00   \n",
      "1  2023-12-15  PARTRP000101238   769,91      260,00   \n",
      "2  2023-12-18  PARTRP000101276   184,62      267,50   \n",
      "3  2023-12-18  PARTRP000101277   202,33      267,50   \n",
      "4  2023-12-19  PARTRP000101305  3160,00      350,00   \n",
      "5  2023-12-19  PARTRP000101326   292,37      256,80   \n",
      "6  2023-12-19  PARTRP000101328   384,46      267,50   \n",
      "7  2023-12-20  PARTRP000101365   418,11      256,80   \n",
      "8  2023-12-20  PARTRP000101366   367,05      267,50   \n",
      "9  2023-12-20  PARTRP000101353  1674,51      374,50   \n",
      "\n",
      "                            filename    Depart   Arrivee  Depart_Lat  \\\n",
      "0  SupplierPreInvoiceSPI00450803.pdf  FR-77185  FR-60300   48.832504   \n",
      "1  SupplierPreInvoiceSPI00450803.pdf  FR-95500  FR-95130   48.985654   \n",
      "2  SupplierPreInvoiceSPI00450803.pdf  FR-95500  FR-77400   48.985654   \n",
      "3  SupplierPreInvoiceSPI00450803.pdf  FR-95500  FR-91310   48.985654   \n",
      "4  SupplierPreInvoiceSPI00450803.pdf  FR-94200  FR-60300   48.815183   \n",
      "5  SupplierPreInvoiceSPI00450803.pdf  FR-95500  FR-91420   48.985654   \n",
      "6  SupplierPreInvoiceSPI00450803.pdf  FR-95500  FR-94210   48.985654   \n",
      "7  SupplierPreInvoiceSPI00450803.pdf  FR-95500  FR-92270   48.985654   \n",
      "8  SupplierPreInvoiceSPI00450803.pdf  FR-95500  FR-78320   48.985654   \n",
      "9  SupplierPreInvoiceSPI00450803.pdf  FR-77170  FR-51100   48.697856   \n",
      "\n",
      "   Depart_Lng  Arrivee_Lat  Arrivee_Lng   Distance  \n",
      "0    2.638544    49.187443     2.664612  24.555845  \n",
      "1    2.464965    48.992035     2.224980  10.922706  \n",
      "2    2.464965    48.880410     2.691282  12.611706  \n",
      "3    2.464965    48.631851     2.268368  26.042441  \n",
      "4    2.384772    49.187443     2.664612  28.698535  \n",
      "5    2.464965    48.702293     2.337153  20.429747  \n",
      "6    2.464965    48.796992     2.512293  13.213921  \n",
      "7    2.464965    48.916262     2.268998  10.126037  \n",
      "8    2.464965    48.736331     1.953673  28.986824  \n",
      "9    2.611796    49.253408     4.048340  75.787289  \n",
      "\n",
      " le  fichier `extracted_data_with_location.csv`  est enregistré!  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "import re\n",
    "\n",
    "class GeoCoder:\n",
    "    def __init__(self, user_agent=\"geo_locator_default\"):\n",
    "        self.geolocator = Nominatim(user_agent = user_agent)\n",
    "        self._coordinates_cache = {} # pour un accès direct des cache (avec une complexité en O(1))\n",
    "\n",
    "    def get_coordinates(self,location):\n",
    "        if location in self._coordinates_cache :\n",
    "            return self._coordinates_cache[location]  \n",
    "       \n",
    "        try:\n",
    "            location_data = self.geolocator.geocode(location)\n",
    "            if location_data : # Si on a un résultats non-null du géocodage \n",
    "                lat = location_data.latitude \n",
    "                lng = location_data.longitude\n",
    "                self._coordinates_cache[location] = (lat,lng) \n",
    "                return lat,lng\n",
    "\n",
    "            else :\n",
    "                self._coordinates_cache[location] = (pd.NA,pd.NA) \n",
    "                return pd.NA, pd.NA\n",
    "        except Exception as e: # capture un problème pour l'appel dans nominatim en interne ou réseau. \n",
    "                self._coordinates_cache[location]  = (pd.NA,pd.NA) # ne remonte pas le bug : continue à avoir l'info NA comme si il n'existait pas et ne bloque pas le programe par exemple\n",
    "                return pd.NA, pd.NA\n",
    "\n",
    "# calul de la distance en mile \n",
    "def calculate_distance(row): \n",
    "    if pd.notna(row['Depart_Lat']) and pd.notna(row['Arrivee_Lat']) and  pd.notna(row['Depart_Lng']) and pd.notna(row['Arrivee_Lng'])  :\n",
    "       coords_1 = (row['Depart_Lat'], row['Depart_Lng'])\n",
    "       coords_2 = (row['Arrivee_Lat'], row['Arrivee_Lng'])\n",
    "       return geodesic(coords_1, coords_2).miles\n",
    "    else :\n",
    "         return pd.NA\n",
    "\n",
    "\n",
    "if __name__ == '__main__': # ce block est lancé si le script est exécuté seul en mode commande\n",
    "  csv_file = r\"C:\\Projet 3\\extracted_data_pdfplumber.csv\" # à remplacer par votre path correct\n",
    "\n",
    "  try:\n",
    "    df = pd.read_csv(csv_file)\n",
    "     # suppression les NAN  dans la colonne Depart/Arrivee: si non supprimé cela met NaN dans pd.NA de géopy\n",
    "    df.dropna(subset =['Depart', 'Arrivee'], inplace = True)\n",
    "    \n",
    "    #extraction  regex  avec le debut (code postale  =>   FR-\\d{5} ) dans la colonne pour garder  les coordonnées de Depart et de Arrivee avec une recheche precise. Pour éviter la prise en compte des valeurs d'avant lors d'un appel sur l'api Geocode (exemple en corrigeant manuelement ou de base les mauvaises coordonnes ou chaine mal convertit avant), on se base en local sur cette valeur unique \n",
    "    df[\"Depart\"]  = df[\"Depart\"].str.extract(r'(FR-\\d{5})' , expand=False) \n",
    "    df[\"Arrivee\"] = df[\"Arrivee\"].str.extract(r'(FR-\\d{5})', expand=False)\n",
    "        \n",
    "    geo_coder_instance = GeoCoder(user_agent = \"custom_geo_agent\") \n",
    "\n",
    "       # Mise en cache  en une ligne: depart, arrivéee dans unique tableau pour avoir uniquement \"une ligne\" (au moment d'envoie en tache, un `map()`) en entrée, et éviter les requetes avec une \"loop imbriqué\" par exemple : plus simple plus lisible et donc maintenable (c'est pour éviter un gros lot de demande) .  Les entrées multiples qui peuvent exister, se servent des clés directment mis en mémoire au cas ou, grâce a l'implémentation par une  dict dans class\n",
    "    all_locations_lists =  df[\"Depart\"].to_list() + df[\"Arrivee\"].to_list()  #  Concaténer tout en 1 liste\n",
    "    locations_coord_all_dicts =  {loc:geo_coder_instance.get_coordinates(loc)    for loc in  all_locations_lists  }\n",
    "  \n",
    "   \n",
    "  #   Assignation  des latitudes/longitudes \n",
    "    df[['Depart_Lat', 'Depart_Lng']] =   pd.DataFrame(   [  locations_coord_all_dicts.get( loc  ,(pd.NA,pd.NA))    for loc  in  df[\"Depart\"]  ]   ,    index= df.index )\n",
    "    df[['Arrivee_Lat', 'Arrivee_Lng']]  =   pd.DataFrame(    [  locations_coord_all_dicts.get(  loc ,  (pd.NA, pd.NA) )     for loc    in df[\"Arrivee\"]  ]     , index = df.index  )\n",
    "\n",
    "   # Calcul simple des distances \n",
    "    df['Distance'] = df.apply(calculate_distance, axis=1)\n",
    "\n",
    "\n",
    "   #Gestion des Erreur sur les cas les plus generaux\n",
    "  except FileNotFoundError as e :\n",
    "        print (f\"Attention fichier introuvable {csv_file} à changer svp !\", e)\n",
    "  except  pd.errors.EmptyDataError as e: \n",
    "          print (f\"Attention le fichier csv ne contient pas d'information: \", e)\n",
    "  except Exception as e:  \n",
    "       print(f\"Attention Une erreur lors de l'éxécution à était detectée , Veuillez verifier: {e} \",e)   \n",
    "       raise\n",
    "  \n",
    "\n",
    "   #Verification de la variable d'ouput df avec les premières lignes si les opérations ci dessus on fonctionné (verifi des types, qu'un traitement  soit ok, ou que ce dernier affiche d'autres problèmes)\n",
    "  if \"df\" in locals() :\n",
    "       print (df.dtypes)   # vérifie l'affchage, et la console les types ou format\n",
    "       print (\"\\nAffichage (les  premières lignes) \") #\n",
    "       print (df.head(10))  \n",
    "   \n",
    "\n",
    "  # enregistrement avec une gestion plus simple. Un csv n'est pas sensé bouger a un meme endroit une fois qu'on l'enregistre)   : si y a probleme l'utilisateur le vera par le print. \n",
    "       \n",
    "  try:\n",
    "       df.to_csv(r'C:\\Projet 3\\extracted_data_with_location.csv' , index=False , encoding=\"utf-8\") \n",
    "       print(\"\\n le  fichier `extracted_data_with_location.csv`  est enregistré!  \")  #  message positif dans la console : permet la lisibilité de l'action par la personne\n",
    "  except Exception as e:\n",
    "     print(f\"\\n Attention un probleme de sauvegarde :\", e ) # Affiche le message si une exception a lieux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exp.Date:</th>\n",
       "      <th>Voyage N°:</th>\n",
       "      <th>Poids:</th>\n",
       "      <th>Montant HT:</th>\n",
       "      <th>filename</th>\n",
       "      <th>Depart</th>\n",
       "      <th>Arrivee</th>\n",
       "      <th>Depart_Lat</th>\n",
       "      <th>Depart_Lng</th>\n",
       "      <th>Arrivee_Lat</th>\n",
       "      <th>Arrivee_Lng</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-12-15</td>\n",
       "      <td>PARTRP000100992</td>\n",
       "      <td>6000,00</td>\n",
       "      <td>350,00</td>\n",
       "      <td>SupplierPreInvoiceSPI00450803.pdf</td>\n",
       "      <td>FR-77185</td>\n",
       "      <td>FR-60300</td>\n",
       "      <td>48.832504</td>\n",
       "      <td>2.638544</td>\n",
       "      <td>49.187443</td>\n",
       "      <td>2.664612</td>\n",
       "      <td>24.555845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-12-15</td>\n",
       "      <td>PARTRP000101238</td>\n",
       "      <td>769,91</td>\n",
       "      <td>260,00</td>\n",
       "      <td>SupplierPreInvoiceSPI00450803.pdf</td>\n",
       "      <td>FR-95500</td>\n",
       "      <td>FR-95130</td>\n",
       "      <td>48.985654</td>\n",
       "      <td>2.464965</td>\n",
       "      <td>48.992035</td>\n",
       "      <td>2.224980</td>\n",
       "      <td>10.922706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-12-18</td>\n",
       "      <td>PARTRP000101276</td>\n",
       "      <td>184,62</td>\n",
       "      <td>267,50</td>\n",
       "      <td>SupplierPreInvoiceSPI00450803.pdf</td>\n",
       "      <td>FR-95500</td>\n",
       "      <td>FR-77400</td>\n",
       "      <td>48.985654</td>\n",
       "      <td>2.464965</td>\n",
       "      <td>48.880410</td>\n",
       "      <td>2.691282</td>\n",
       "      <td>12.611706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-12-18</td>\n",
       "      <td>PARTRP000101277</td>\n",
       "      <td>202,33</td>\n",
       "      <td>267,50</td>\n",
       "      <td>SupplierPreInvoiceSPI00450803.pdf</td>\n",
       "      <td>FR-95500</td>\n",
       "      <td>FR-91310</td>\n",
       "      <td>48.985654</td>\n",
       "      <td>2.464965</td>\n",
       "      <td>48.631851</td>\n",
       "      <td>2.268368</td>\n",
       "      <td>26.042441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-12-19</td>\n",
       "      <td>PARTRP000101305</td>\n",
       "      <td>3160,00</td>\n",
       "      <td>350,00</td>\n",
       "      <td>SupplierPreInvoiceSPI00450803.pdf</td>\n",
       "      <td>FR-94200</td>\n",
       "      <td>FR-60300</td>\n",
       "      <td>48.815183</td>\n",
       "      <td>2.384772</td>\n",
       "      <td>49.187443</td>\n",
       "      <td>2.664612</td>\n",
       "      <td>28.698535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Exp.Date:       Voyage N°:   Poids: Montant HT:  \\\n",
       "0  2023-12-15  PARTRP000100992  6000,00      350,00   \n",
       "1  2023-12-15  PARTRP000101238   769,91      260,00   \n",
       "2  2023-12-18  PARTRP000101276   184,62      267,50   \n",
       "3  2023-12-18  PARTRP000101277   202,33      267,50   \n",
       "4  2023-12-19  PARTRP000101305  3160,00      350,00   \n",
       "\n",
       "                            filename    Depart   Arrivee  Depart_Lat  \\\n",
       "0  SupplierPreInvoiceSPI00450803.pdf  FR-77185  FR-60300   48.832504   \n",
       "1  SupplierPreInvoiceSPI00450803.pdf  FR-95500  FR-95130   48.985654   \n",
       "2  SupplierPreInvoiceSPI00450803.pdf  FR-95500  FR-77400   48.985654   \n",
       "3  SupplierPreInvoiceSPI00450803.pdf  FR-95500  FR-91310   48.985654   \n",
       "4  SupplierPreInvoiceSPI00450803.pdf  FR-94200  FR-60300   48.815183   \n",
       "\n",
       "   Depart_Lng  Arrivee_Lat  Arrivee_Lng   Distance  \n",
       "0    2.638544    49.187443     2.664612  24.555845  \n",
       "1    2.464965    48.992035     2.224980  10.922706  \n",
       "2    2.464965    48.880410     2.691282  12.611706  \n",
       "3    2.464965    48.631851     2.268368  26.042441  \n",
       "4    2.384772    49.187443     2.664612  28.698535  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.read_csv('extracted_data_with_location.csv')\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Log et Analyse  pour   : C:\\Projet 3\\extracted_data_pdfplumber.csv =======  \n",
      "\n",
      "Data Types (de Pandas sur le DF extrait de python) \n",
      "\n",
      "Exp.Date:              object\n",
      "Voyage N°:             object\n",
      "Poids:                 object\n",
      "Montant HT:            object\n",
      "filename               object\n",
      "Depart                 object\n",
      "Arrivee                object\n",
      "Depart_Lat            float64\n",
      "Depart_Lng            float64\n",
      "Arrivee_Lat           float64\n",
      "Arrivee_Lng           float64\n",
      "Distance              float64\n",
      "DistanceKM            float64\n",
      "Conso ( litre )       float64\n",
      "Cout total (euros)    float64\n",
      "dtype: object\n",
      "\n",
      " Statistiques via describe()  :\n",
      "\n",
      "       Depart_Lat  Depart_Lng  Arrivee_Lat  Arrivee_Lng     Distance  \\\n",
      "count  797.000000  797.000000   797.000000   797.000000   797.000000   \n",
      "mean    49.010394    2.523770    48.791829     2.445864    42.252112   \n",
      "std      0.211179    0.307093     0.451234     4.281818   178.121396   \n",
      "min     47.926260   -3.689389    45.893352  -115.457456     0.000000   \n",
      "25%     48.985654    2.464965    48.618553     2.248881    15.770504   \n",
      "50%     48.985654    2.464965    48.833104     2.499842    26.326124   \n",
      "75%     49.003394    2.638544    48.989054     2.664612    40.725765   \n",
      "max     49.823092    3.283638    50.380581     7.166015  4909.927786   \n",
      "\n",
      "        DistanceKM  Conso ( litre )  Cout total (euros)  \n",
      "count   797.000000       797.000000          797.000000  \n",
      "mean     67.998014        22.439345           40.390820  \n",
      "std     286.657887        94.597103          170.274785  \n",
      "min       0.000000         0.000000            0.000000  \n",
      "25%      25.380103         8.375434           15.075781  \n",
      "50%      42.367684        13.981336           25.166404  \n",
      "75%      65.541602        21.628729           38.931712  \n",
      "max    7901.743184      2607.575251         4693.635451  \n",
      "\n",
      " Vérifications NAN en fin  ,  pour voir le debug plus haut, en total  de toutes valeurs, par Colonnes, lors du parsing \n",
      "\n",
      "Exp.Date:             0\n",
      "Voyage N°:            0\n",
      "Poids:                0\n",
      "Montant HT:           0\n",
      "filename              0\n",
      "Depart                0\n",
      "Arrivee               0\n",
      "Depart_Lat            0\n",
      "Depart_Lng            0\n",
      "Arrivee_Lat           0\n",
      "Arrivee_Lng           0\n",
      "Distance              0\n",
      "DistanceKM            0\n",
      "Conso ( litre )       0\n",
      "Cout total (euros)    0\n",
      "dtype: int64\n",
      " \n",
      " fichier 'extracted_data_with_location.csv' à été enregistré correctement (utf-8) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "import re\n",
    "\n",
    "class GeoCoder:\n",
    "    def __init__(self, user_agent=\"geo_locator_default\"):\n",
    "        self.geolocator = Nominatim(user_agent = user_agent)\n",
    "        self._coordinates_cache = {}\n",
    "\n",
    "    def get_coordinates(self,location):\n",
    "       if location in self._coordinates_cache:\n",
    "         return self._coordinates_cache[location]\n",
    "       \n",
    "       try:\n",
    "           location_data = self.geolocator.geocode(location)\n",
    "           if location_data :\n",
    "               lat = location_data.latitude\n",
    "               lng= location_data.longitude\n",
    "               self._coordinates_cache[location]  = (lat,lng)\n",
    "               return lat,lng\n",
    "           else :\n",
    "                self._coordinates_cache[location]  = (pd.NA,pd.NA)\n",
    "                return pd.NA, pd.NA\n",
    "       except Exception as e :\n",
    "         self._coordinates_cache[location] = (pd.NA,pd.NA)\n",
    "         return pd.NA,pd.NA\n",
    "\n",
    "def calculate_distance(row):\n",
    "    if pd.notna(row['Depart_Lat']) and pd.notna(row['Arrivee_Lat']) and  pd.notna(row['Depart_Lng']) and pd.notna(row['Arrivee_Lng'])  :  \n",
    "        coords_1 = (row['Depart_Lat'], row['Depart_Lng'])\n",
    "        coords_2 = (row['Arrivee_Lat'], row['Arrivee_Lng'])\n",
    "        return geodesic(coords_1, coords_2).miles\n",
    "    else:\n",
    "        return pd.NA\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    csv_file = r\"C:\\Projet 3\\extracted_data_pdfplumber.csv\"  # remplacer ici\n",
    "   \n",
    "    try :\n",
    "         df = pd.read_csv(csv_file)\n",
    "        \n",
    "         df.dropna(subset =['Depart', 'Arrivee'], inplace = True)\n",
    "\n",
    "          #regex pour la suppression de texte avec des codes postaux dans chaine et uniquement : important \n",
    "         df[\"Depart\"]  = df[\"Depart\"].str.extract(r'(FR-\\d{5})', expand = False)\n",
    "         df[\"Arrivee\"] = df[\"Arrivee\"].str.extract(r'(FR-\\d{5})' , expand = False)\n",
    "\n",
    "\n",
    "         geo_coder_instance = GeoCoder(user_agent = \"custom_geo_agent\")\n",
    "            # la map : tous  via une unique et seul ligne et via dict (voir code precedent pour memoire ou on recup /  geolocaliste, et le system de cache des variables ):  le point cle reste l'entree en parametres \"list unique/variable unique : c'est  toujours la \"transformation masse /par lot qu'on fait\", meme quand  il faut recharger tout ou  qu'il n'y ai pas  de data. Un autre choix restes des lambdas (un by row qui execute par l'API chaque location , l'impact reste fort par l'envoie dans la phase des tests)) . C'est plus optimisé, et plus maintenable, via liste de  strings pour le code  (tous est la dans all_location): cela evite beaucoup le debugger sur plusier  elements\" imbriquer lors de future manipulations \"a cet endroit (par exemple) sur pandas lors d'appel vers  d'autre outils\n",
    "         all_locations_lists =  df[\"Depart\"].to_list()  +  df[\"Arrivee\"].to_list()\n",
    "         locations_coord_all_dicts =  { loc : geo_coder_instance.get_coordinates(loc)    for loc   in   all_locations_lists  } # \n",
    "\n",
    "        \n",
    "        #  assignations variables \n",
    "         df[['Depart_Lat', 'Depart_Lng']]  = pd.DataFrame(  [locations_coord_all_dicts.get(loc ,  (pd.NA,pd.NA)  )   for loc in  df[\"Depart\"]] , index=df.index)\n",
    "         df[['Arrivee_Lat', 'Arrivee_Lng']] = pd.DataFrame( [locations_coord_all_dicts.get(loc  ,(pd.NA,pd.NA))    for loc in df['Arrivee'] ] ,index=df.index  )\n",
    "\n",
    "         # Calcul distance simple . . le reste est par des transformation directes par pandas ou calcul sans boucle: reste beaucoup plus rapide que via un   `map(  lambdas x : ...) : . La partie Distance est obligatoire et sert lors du calcul de Consommation + Couts`\n",
    "\n",
    "         df['Distance'] = df.apply(calculate_distance, axis = 1)\n",
    "\n",
    "\n",
    "          # ajout des nouveaux champ via opération  : \"convert miles -> km \", `distance (Km), ` \"conso(litres), `  cout totals  (euros) ` \"\n",
    "         df['DistanceKM'] =  df['Distance']  * 1.60934 \n",
    "         df['Conso ( litre )'] = df['DistanceKM'] * 0.33\n",
    "         df['Cout total (euros)'] = df['Conso ( litre )']  * 1.80\n",
    "\n",
    "         #Gestion d'erreur. Dans cette  partie du dev :  les \"exceptions\", on met toujours : FileNotFOund + Empty : la ligne doit toujours avoir son  rôle à jouer car Power bi si le code fonctionne  doit fonctionner aussi : une coherence visuel \"a partir de la table  vers PowerBi  \". \n",
    "\n",
    "    except FileNotFoundError as e : # Message d'erreure que c'est un fichier en csv qu'est absent (important pour controler dans le workflow de Python, si la creation du  Csv depuis python   est la source d'erreur . Vous pouvez adapter votre log en fonction ) \n",
    "          print(f\"Fichier CSV introuvable à modifier:{csv_file}, SVP relancez, ou mettez votre chemi\", e) \n",
    "    except pd.errors.EmptyDataError as e : # un msg de logs: votre csv (qui a produit toute cette operations sur mes operations pandas a des chaines de fonctions/methode. Ca se teste en  ajoutant tous le DTypes et la  description avant un retour , meme au final via print\n",
    "        print(f\"Votre fichier  '{csv_file}' ,  ne contenait aucune informations !\", e) # msg\n",
    "    except Exception as e : \n",
    "       print (f\" Un soucis a été détectée via Pandas. Veillez verifier {e}\")  \n",
    "       raise\n",
    "   \n",
    "\n",
    "   # partie des  transformations et \"des erreurs potentielles \": l'output / le dataframe pour pouvoir a l'étape   \"\" PowerBI\"\"\"   de diagnostiquer d'autres eventuel problème (sur la source ) . et je vise surtout les cas numériques et la compatibilité : le but de mettre une operation pandas sur toute l'etendue est aussi une verif  pour le test du CSV en une version stable , tel  quel) : car un data analysis c'est la mise en relation data (source et ouput). C'est dans  tous le fil, ou j'essais de valider\n",
    "    if \"df\" in locals()  :   # Si dataframe a ete créé:  on test  et affiche , pour verifié: les nouvelles col avec NaN, ou via dtype : avec un mode describe, (pour eviter les surprise dans power BI: car là , on sort en \" float\" au point a l'action avec Power Query via les  `to_numberic()` dans la librairie python  + .replace (que j'avais du mettre précédemment à cause de bug mais ce n'est plus necessaire, une selection precise doit fonctionner direct lors d'un simple  tri . Le NAN , ici est l'info, ou  on sait que la conversion en numberique s'est mal passee a l'etape precedentes ). La sortie console de df et dtypes a une grande importanc à mes yeux, en terme de robustesses du process sur lequel je teste tous \n",
    "   \n",
    "        print(f\"======== Log et Analyse  pour   : {csv_file} =======  \") # un debut ( pour un  affichage unique lors  de la data: tout le bloc de verification, avant le CSV\n",
    "\n",
    "          # la version data_types via pandas\n",
    "        print(\"\\nData Types (de Pandas sur le DF extrait de python) \\n\") #\n",
    "        print (df.dtypes) #   Le type d'une variables  ou formatage doit ressortir, \" via le debug\", des chaines \"\"  et non int , voir float en mode objet : cela permet dans \" ce \" debug \"\" de   verifi \" qu'on controle\",  de mes recommendation dans une Data (c'est le \"nan  qu'on va \"forcer en valeur \" au debut via pandas au lieu qu'un   type chaine par exemples lors d'un read )\n",
    "        print (\"\\n Statistiques via describe()  :\\n\") # la sortie en stats donne tous  en  resume + l'information et a verif via data (ou voir si  certaines données sont cohérente : des chiffres, des strings etc ..)  des col numériques et des format types : a  controlez aussi dans une étape avant PowerBI . Utile lors du débogage. Pour l'analyse sur certaines colonnes (on est  sur le max / le min si tous fait sens ou non a des moments du traitement du dataset brut  a l'export. Et donc de diagnostiqué le plus rapidemment l'etapes ou  les données ne corespondait plus avant  même  dans power BI). Il doit rester  clair. Et doit faire \"reflet a toutes manipulations du dataframe dans toutes etapes , pour debugger la provenance (ex la variable  montant qui ne devrait plus faire apparaitre nan apres conversion en float en bas des col)\n",
    "        print (df.describe())\n",
    "\n",
    "          # La nouvelle partie en mode Nan qui devra s'afficher\n",
    "        print (f\"\\n Vérifications NAN en fin  ,  pour voir le debug plus haut, en total  de toutes valeurs, par Colonnes, lors du parsing \\n\")# Verification \"\" rapide \"\" que tous c'est ok , l'utilisateur / dev doit pouvoir s'attendre de cette information / log sur ses colonnes qui reste dans sa visualisation dans powerBI\n",
    "        print (df.isna().sum())  # ici nous cherchons si malgres tous ,  des valeurs   NaN  arrive dans un point \"non prévus\" pour  la colonne qu'elle reste bien  avec \" 0\", afin que le  graphe / select / calcul en interne \"power BI\" fasse que le job dans un contexte  du filtrage a la selections , des  actions  (enfin tous un workflows), des manipulations des donnés  dans un dashboards !  Vous  aussi pouvez verifié tous celas.  Le probleme et les \"causes\"  avec cet action de NAN /NULL doit se detecter avec l'interface d'affichage / graph : via un filtre (date et selections multiple sur d'autre categorie), et que cela donne toujours un resulta   :  si pas cas . c'est  ici que  on pourra debugger ! Et surtout si lors de mes transformation: vous n'arrivez pas à comprendre comment une chaine est passé  a du \" nan\" ou si mal convertit la phase amont, doit toujours  laissé  une bonne info\n",
    "\n",
    "      # sauvegarde ,   via dataframe et test\n",
    "        try : # je fais un essai via un to csv. Cette partie en python se fait bien. En général les erreur sur \"read  dans des fichiers qui excedes une limite mémoire, ou  format \" non correct\" apparaissent dans une lecture : d'où   , la  reverification pour vous de ne pas \" avoir à chercher à l'étap Power BI , dans cette dernière lecture ou l'import reste la  dernière etapes \" qui valide, votre projet, par toutes mes étapes). D'où le utf-8 pour que des csv/ powerBi s'affich  au max : un message + erreur doit faire reagir la personne sur une chaine de  traitement (pas les  \"dizaine /centaines  de visualisatoin en plus\". Une verification simple / rapide via test par le haut (lors de \"test et de feedback par le log\"), est très pertinent . On doit   identifier à ce moment \" le maillon faible \" si jamais ce soucis (NaN / chaine et non numeric\") remonte encore et via toute  transformations faites )\n",
    "\n",
    "         df.to_csv(r\"C:\\Projet 3\\extracted_data_with_location.csv\" , encoding=\"utf-8\" , index=False ) # utf8, sans index en début (reste coherent avec les autres fichiers/data csv si une relation multiple  est faite) : afin de valider de  mes transformation des variables du data frames qui fait transiter un csv via python et toutes mes actions. Sans aucune transformation sur les data. La manipulation de python ne change \"pas l'entree\"\" en general lors d'import pandas : donc toutes modif  PowerBI et leurs transformation se veront si y avait \" un manque  a  la  relecture en fin des traitements \" en fonction des étapes faites : c'est toute la logiques en termes de \" chainage de données \". Pour pouvoir aussi localiser/trouvé plus  aisement, le moindre probleme de format, types (si via  to_numeric et que vous affiche du Nan alors que tous est corrigé).   Un bon export  en python garanti le bon fonctionement a tous moments: ce qu'on attendait.    En resumer, il  vaut mieux aussi \"une verification / traitement local via une donnée du debut jusqu'à la sortie\". . J'espère à cet instant la vous arrivez un CSV a tous points irréprochable en amont, par \" l'appel\" a un `to_numeric`  qui valide toutes actions du dataframes a  ce moment (puis l'affchage) afin que PowerBI , prenne la suite a présent sans trop de souci et plus rapidement.  Le `type` reste primordial !. (toute  erreur au moment des visual : devras vous  faire reagir a cette chaine ou des  données \"nan et incorrect de la base du code d'avant\" n'etait pas traité  en consequence: dans un vrai projet une donnée qui se propage en \"NaN \" , restes bien plus problématiques , car vous ne savez pas la source !\n",
    "\n",
    "         print (\" \\n fichier 'extracted_data_with_location.csv' à été enregistré correctement (utf-8) \\n\"  )\n",
    "\n",
    "        except  Exception as e: \n",
    "            print (f\"\\n Lors de la  sauvegarde  de '{csv_file}' (avec to csv  ) une exception a été attrapé : veuillez verifié. \", e) # la partie sauvegarde est important pour ne pas avoir les probleme du   style d'ou ca  plante  via un fichier csv deja existant\n",
    "            raise  # ca relève une error avec traceback,  mais l'utilisateur aura des traces a inspecté. Permet d'un dévelopeur  à déboogué\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
